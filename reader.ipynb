{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.status.busy":"2022-02-08T07:05:00.966161Z","iopub.status.idle":"2022-02-08T07:05:00.966777Z","shell.execute_reply":"2022-02-08T07:05:00.966572Z","shell.execute_reply.started":"2022-02-08T07:05:00.966548Z"},"id":"Ifq5xlTrsc7B","outputId":"48823469-9e7f-47e2-979c-02fff9911e79","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets==1.17.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (1.17.0)\n","Requirement already satisfied: transformers==4.15.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (4.15.0)\n","Requirement already satisfied: tokenizers in d:\\anaconda3\\envs\\qa\\lib\\site-packages (0.10.3)\n","Requirement already satisfied: tqdm>=4.62.1 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (4.62.3)\n","Requirement already satisfied: dataclasses in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (0.8)\n","Requirement already satisfied: xxhash in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (2.0.2)\n","Requirement already satisfied: requests>=2.19.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (2.27.1)\n","Requirement already satisfied: dill in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (0.3.4)\n","Requirement already satisfied: aiohttp in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (3.8.1)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (6.0.1)\n","Requirement already satisfied: packaging in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (21.3)\n","Requirement already satisfied: multiprocess in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (0.70.12.2)\n","Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (1.19.5)\n","Requirement already satisfied: pandas in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (1.1.5)\n","Requirement already satisfied: importlib-metadata in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (4.8.3)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (0.4.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from datasets==1.17.0) (2022.1.0)\n","Requirement already satisfied: sacremoses in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from transformers==4.15.0) (0.0.47)\n","Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from transformers==4.15.0) (6.0)\n","Requirement already satisfied: filelock in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from transformers==4.15.0) (3.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from transformers==4.15.0) (2022.1.18)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.17.0) (4.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from packaging->datasets==1.17.0) (3.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from requests>=2.19.0->datasets==1.17.0) (2021.5.30)\n","Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from requests>=2.19.0->datasets==1.17.0) (3.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from requests>=2.19.0->datasets==1.17.0) (2.0.11)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from requests>=2.19.0->datasets==1.17.0) (1.26.8)\n","Requirement already satisfied: colorama in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from tqdm>=4.62.1->datasets==1.17.0) (0.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from aiohttp->datasets==1.17.0) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from aiohttp->datasets==1.17.0) (21.4.0)\n","Requirement already satisfied: idna-ssl>=1.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from aiohttp->datasets==1.17.0) (1.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from aiohttp->datasets==1.17.0) (1.7.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from aiohttp->datasets==1.17.0) (5.2.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from aiohttp->datasets==1.17.0) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from aiohttp->datasets==1.17.0) (0.13.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from aiohttp->datasets==1.17.0) (1.2.0)\n","Requirement already satisfied: zipp>=0.5 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from importlib-metadata->datasets==1.17.0) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from pandas->datasets==1.17.0) (2021.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from pandas->datasets==1.17.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.17.0) (1.16.0)\n","Requirement already satisfied: joblib in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from sacremoses->transformers==4.15.0) (1.1.0)\n","Requirement already satisfied: click in d:\\anaconda3\\envs\\qa\\lib\\site-packages (from sacremoses->transformers==4.15.0) (8.0.3)\n"]}],"source":["! pip  install datasets==1.17.0 transformers==4.15.0 tokenizers"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T10:19:40.977153Z","iopub.status.busy":"2022-02-08T10:19:40.976528Z","iopub.status.idle":"2022-02-08T10:19:58.408550Z","shell.execute_reply":"2022-02-08T10:19:58.407704Z","shell.execute_reply.started":"2022-02-08T10:19:40.977115Z"},"id":"Q7SX18zHsLz1","outputId":"1d0cf4e4-b33d-4157-efae-478e2ead2341","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Reusing dataset squad_v2 (C:\\Users\\Admin\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['id', 'title', 'context', 'question', 'answers'],\n","    num_rows: 130319\n","})"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","squad = load_dataset('squad_v2', split='train')\n","squad"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T10:20:01.843551Z","iopub.status.busy":"2022-02-08T10:20:01.843079Z","iopub.status.idle":"2022-02-08T10:20:07.662765Z","shell.execute_reply":"2022-02-08T10:20:07.662076Z","shell.execute_reply.started":"2022-02-08T10:20:01.843512Z"},"id":"BbSHoYzAtoOb","outputId":"9a3fe40a-0122-4143-c130-0e55a19b67fb","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Admin/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Admin/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Admin/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Admin/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]},{"ename":"AttributeError","evalue":"'BertTokenizerFast' object has no attribute 'to'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-14-14fcaf632815>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mAttributeError\u001b[0m: 'BertTokenizerFast' object has no attribute 'to'"]}],"source":["from transformers import BertTokenizerFast\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T10:20:07.664861Z","iopub.status.busy":"2022-02-08T10:20:07.664516Z","iopub.status.idle":"2022-02-08T10:38:29.121598Z","shell.execute_reply":"2022-02-08T10:38:29.120828Z","shell.execute_reply.started":"2022-02-08T10:20:07.664822Z"},"id":"Dnwzl8vtttsZ","outputId":"e42635bd-bbcd-49b6-a5bb-5e583c387d37","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at C:\\Users\\Admin\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\\cache-d47fd42753b517a7.arrow\n"]}],"source":["squad = squad.map(lambda x: tokenizer(\n","    x['question'], x['context'], max_length=384,\n","    padding='max_length', truncation=True,\n","    return_offsets_mapping=True\n","))"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T10:38:29.123681Z","iopub.status.busy":"2022-02-08T10:38:29.123209Z","iopub.status.idle":"2022-02-08T10:38:29.945041Z","shell.execute_reply":"2022-02-08T10:38:29.944331Z","shell.execute_reply.started":"2022-02-08T10:38:29.123637Z"},"id":"Nw8UgKO4tve1","outputId":"01f84237-793e-488b-b2d0-db6a7ab55215","trusted":true},"outputs":[{"data":{"text/plain":["'[CLS] when did beyonce start becoming popular? [SEP] beyonce giselle knowles - carter ( / biːˈjɒnseɪ / bee - yon - say ) ( born september 4, 1981 ) is an american singer, songwriter, record producer and actress. born and raised in houston, texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of r & b girl - group destiny\\'s child. managed by her father, mathew knowles, the group became one of the world\\'s best - selling girl groups of all time. their hiatus saw the release of beyonce\\'s debut album, dangerously in love ( 2003 ), which established her as a solo artist worldwide, earned five grammy awards and featured the billboard hot 100 number - one singles \" crazy in love \" and \" baby boy \". [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(squad[0]['input_ids'])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T10:38:29.947387Z","iopub.status.busy":"2022-02-08T10:38:29.946976Z","iopub.status.idle":"2022-02-08T10:38:29.961056Z","shell.execute_reply":"2022-02-08T10:38:29.960362Z","shell.execute_reply.started":"2022-02-08T10:38:29.947347Z"},"id":"J0tnhaDkt42r","outputId":"0c9ba7d8-5dc7-4995-bf5a-3c769c1b7be1","trusted":true},"outputs":[{"data":{"text/plain":["(9, 165)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["question_len = 0\n","for x in squad[0]['token_type_ids']:\n","    if x != 1:\n","        question_len += 1\n","    else: break\n","context_len = sum(squad[0]['token_type_ids'])\n","question_len, context_len"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T10:38:29.962617Z","iopub.status.busy":"2022-02-08T10:38:29.962354Z","iopub.status.idle":"2022-02-08T10:38:29.971692Z","shell.execute_reply":"2022-02-08T10:38:29.970790Z","shell.execute_reply.started":"2022-02-08T10:38:29.962582Z"},"id":"KASMBMlnt9dT","trusted":true},"outputs":[],"source":["def char_to_id(sample):\n","    # find the question length\n","    try:\n","        char_start = sample['answers']['answer_start'][0]\n","        sample['answers']['answer_end'] = sample['answers']['answer_start'][0] + len(sample['answers']['text'][0])\n","        char_end = sample['answers']['answer_end']\n","    except:\n","        char_start = 0\n","        char_end = 0\n","    question_len = 0\n","    for x in sample['token_type_ids']:\n","        if x != 1:\n","            question_len += 1\n","        else: break\n","    # and get the context length\n","    context_len = sum(sample['token_type_ids'])\n","    # get offset mappings for context segment\n","    context_mappings = sample['offset_mapping'][question_len:][:context_len-1]\n","    for i, mapping in enumerate(context_mappings):\n","        if char_start >= mapping[0] and char_start <= mapping[1]:\n","            token_start = question_len + i\n","        if char_end >= mapping[0] and char_end <= mapping[1]:\n","            token_end = question_len + i + 1\n","            return {'start_positions': token_start, 'end_positions': token_end}\n","        if i == len(context_mappings) - 1:\n","            # this means the answer tokens are out of range, eg have been truncated\n","            # and therefore there is no answer\n","            token_start, token_end = 0, 0\n","            return {'start_positions': token_start, 'end_positions': token_end}"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T10:38:29.973393Z","iopub.status.busy":"2022-02-08T10:38:29.972928Z","iopub.status.idle":"2022-02-08T10:45:24.936207Z","shell.execute_reply":"2022-02-08T10:45:24.935481Z","shell.execute_reply.started":"2022-02-08T10:38:29.973356Z"},"id":"yCpYQzafuHDk","outputId":"bb5d7cd8-e2b4-4fef-9a9c-512c70932d95","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at C:\\Users\\Admin\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\\cache-5d767338c1e6104b.arrow\n"]}],"source":["squad = squad.map(lambda x: char_to_id(x))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T10:45:24.939609Z","iopub.status.busy":"2022-02-08T10:45:24.939334Z","iopub.status.idle":"2022-02-08T10:45:24.950070Z","shell.execute_reply":"2022-02-08T10:45:24.949471Z","shell.execute_reply.started":"2022-02-08T10:45:24.939579Z"},"id":"_xHEHiV6yNAd","outputId":"ae4f83d5-8cba-408f-cd0d-ae553b7afded","trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n","    num_rows: 130319\n","})"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["squad = squad.remove_columns(['id', 'title', 'context', 'question', 'answers', 'offset_mapping'])\n","squad"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T10:45:24.953504Z","iopub.status.busy":"2022-02-08T10:45:24.952919Z","iopub.status.idle":"2022-02-08T10:45:38.816098Z","shell.execute_reply":"2022-02-08T10:45:38.815414Z","shell.execute_reply.started":"2022-02-08T10:45:24.953466Z"},"id":"IQTJsfYMuNh4","outputId":"8780d2fd-27de-41d0-fa20-6b86ae58881d","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import default_data_collator\n","from transformers import BertForQuestionAnswering\n","data_collator = default_data_collator\n","model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["from transformers import TrainingArguments\n","\n","batch_size = 18\n","epochs = 1\n","\n","args = TrainingArguments(\n","    'bert-base-uncased-qa-squad2',\n","    learning_rate=3e-5,\n","    per_device_train_batch_size=batch_size,\n","    num_train_epochs=epochs,\n","    weight_decay=0.1,\n","    warmup_steps=int(len(squad)*epochs*0.1))\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 130319\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 18\n","  Total train batch size (w. parallel, distributed & accumulation) = 18\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7240\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 4.00 GiB total capacity; 2.98 GiB already allocated; 0 bytes free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m<ipython-input-18-e86468a62639>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1330\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1332\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m                 if (\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1922\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1924\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1925\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1846\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         )\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    992\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m         encoder_outputs = self.encoder(\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    158\u001b[0m         return F.embedding(\n\u001b[0;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Anaconda3\\envs\\qa\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 4.00 GiB total capacity; 2.98 GiB already allocated; 0 bytes free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["from transformers import Trainer\n","import torch\n","torch.cuda.empty_cache()\n","\n","# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","device = 'cpu'\n","trainer = Trainer(\n","    model.to(\"cpu\"),\n","    args,\n","    train_dataset=squad,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXwWF6ppvRXr"},"outputs":[],"source":["trainer.save_model('bert-base-uncased-qa-squad2')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"}},"nbformat":4,"nbformat_minor":4}
